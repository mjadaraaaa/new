{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a87681b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "N=1000 | y∈[0,1] | S(min,max) among known=0.000/1.000\n",
      "Observed (train) pairs=44955 | Holdout (pairs)=4995 | Unobserved=449550\n",
      "#Atoms found: 30\n",
      "Fingerprint shape: (1000, 264)\n"
     ]
    }
   ],
   "source": [
    "# ================== ONE CELL: C-simple + FP→Z + Truth ==================\n",
    "# If needed (Colab): !pip -q install numpy pandas scipy scikit-learn torch\n",
    "import os, re, random, hashlib, numpy as np, pandas as pd\n",
    "\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import torch, torch.nn as nn\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "CSV_PATH           = r\"C:\\Users\\pc\\Desktop\\30_1000_base.csv\"  # first column y, then N formula columns (S)\n",
    "SEED               = 7\n",
    "DIM                = 128               # embedding dim for spectral Z and student\n",
    "M_PROBES           = 256               # # of semantic mini-worlds (fingerprint)\n",
    "OBS_FRACTION       = 0.10              # used only if CSV has no missing entries (to simulate partial observation)\n",
    "HOLDOUT_FRACTION   = 0.10              # fraction of observed pairs kept for leak-free eval of completion\n",
    "PAIR_EVAL          = 20000             # #pairs to sample for correlation sanity checks\n",
    "# GNN (C-simple)\n",
    "EPOCHS_C           = 3000\n",
    "BATCH_EDGES_C      = 40000\n",
    "LR_C               = 1e-3\n",
    "APPNP_K            = 10\n",
    "APPNP_ALPHA        = 0.1\n",
    "EDGE_TEMP          = 1.0               # keep 1.0 for pure regression; 1.5–2.0 to sharpen propagation later\n",
    "BLOCK_PRED         = 128               # block size for full kernel prediction\n",
    "# Student (FP→Z)\n",
    "EPOCHS_STUDENT     = 40\n",
    "LR_STUDENT         = 2e-3\n",
    "PAIR_SAMPLES       = 2048              # pairwise cosine samples per batch\n",
    "PAIR_LOSS_W        = 0.5               # weight for pairwise loss vs vector MSE\n",
    "BATCH_STUDENT      = 512\n",
    "# Truth head\n",
    "EPOCHS_TRUTH       = 20\n",
    "LR_TRUTH           = 1e-3\n",
    "BATCH_TRUTH        = 512\n",
    "# Saving\n",
    "SAVE_ARTIFACTS     = True\n",
    "OUT_DIR            = \"./\"\n",
    "\n",
    "# ---------------- Repro & Device ----------------\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# ---------------- Load CSV ----------------\n",
    "assert os.path.exists(CSV_PATH), f\"File not found: {CSV_PATH}\"\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "N  = df.shape[0]\n",
    "formulas = list(df.columns[1:])\n",
    "assert len(formulas) == N, \"Expected N rows and N formula columns (1..N).\"\n",
    "y = df.iloc[:, 0].to_numpy().astype(np.float32)\n",
    "S_raw = df.iloc[:, 1:].to_numpy(dtype=float)  # may include NaN for unobserved pairs\n",
    "\n",
    "# Ensure symmetry (even if sparse) but keep NaNs where both missing\n",
    "S = 0.5*(S_raw + S_raw.T)\n",
    "diag = np.eye(N, dtype=bool)\n",
    "S[diag] = 1.0\n",
    "# Clip known entries into [0,1]\n",
    "mask_known = ~np.isnan(S)\n",
    "S[mask_known] = np.clip(S[mask_known], 0.0, 1.0)\n",
    "y_bin = (y > 0.5).astype(np.float32)\n",
    "print(f\"N={N} | y∈[0,1] | S(min,max) among known={np.nanmin(S):.3f}/{np.nanmax(S):.3f}\")\n",
    "\n",
    "# ---------------- Build observed/unobserved/holdout pairs ----------------\n",
    "def upper_pairs(N):\n",
    "    # return all (i,j) with i<j\n",
    "    I, J = np.triu_indices(N, k=1)\n",
    "    return np.stack([I, J], axis=1)\n",
    "\n",
    "all_pairs_u = upper_pairs(N)\n",
    "\n",
    "# If the CSV is dense (no NaNs), simulate a partial observation set\n",
    "if np.isnan(S).sum() == 0:\n",
    "    total = len(all_pairs_u)\n",
    "    m_obs = max(1, int(OBS_FRACTION * total))\n",
    "    idx = np.random.RandomState(SEED).choice(total, size=m_obs, replace=False)\n",
    "    obs_pairs_all = all_pairs_u[idx]\n",
    "    obs_vals_all  = S[obs_pairs_all[:,0], obs_pairs_all[:,1]]\n",
    "else:\n",
    "    # Use only pairs with known values (i<j)\n",
    "    Kmask = (~np.isnan(S)) & (~np.eye(N, dtype=bool))\n",
    "    I, J = np.where(np.triu(Kmask, k=1))\n",
    "    obs_pairs_all = np.stack([I, J], axis=1)\n",
    "    obs_vals_all  = S[I, J].astype(np.float32)\n",
    "\n",
    "# Split observed into train-observed vs holdout-observed for leak-free completion eval\n",
    "rng = np.random.RandomState(SEED)\n",
    "perm = rng.permutation(len(obs_pairs_all))\n",
    "m_hold = max(1, int(HOLDOUT_FRACTION * len(obs_pairs_all)))\n",
    "hold_idx = perm[:m_hold]; train_obs_idx = perm[m_hold:]\n",
    "holdout_pairs = obs_pairs_all[hold_idx]\n",
    "holdout_true  = obs_vals_all[hold_idx]\n",
    "obs_pairs     = obs_pairs_all[train_obs_idx]\n",
    "obs_vals      = obs_vals_all[train_obs_idx]\n",
    "\n",
    "# Unobserved = all remaining upper pairs not in obs_pairs_all\n",
    "obs_set = set(map(tuple, obs_pairs_all.tolist()))\n",
    "unobs_pairs = np.array([p for p in all_pairs_u.tolist() if tuple(p) not in obs_set], dtype=np.int64)\n",
    "\n",
    "print(f\"Observed (train) pairs={len(obs_pairs)} | Holdout (pairs)={len(holdout_pairs)} | Unobserved={len(unobs_pairs)}\")\n",
    "\n",
    "# ---------------- Propositional Parser & Robust FP (unseen atoms handled) ----------------\n",
    "OP_MAP = {\"→\":\" IMP \", \"⇒\":\" IMP \", \"=>\":\" IMP \", \"->\":\" IMP \",\n",
    "          \"↔\":\" IFF \", \"<=>\":\" IFF \", \"<->\":\" IFF \",\n",
    "          \"⊑\":\" SUB \",  # treat as IMP\n",
    "          \"⊓\":\" AND \", \"∧\":\" AND \", \"&&\":\" AND \",\n",
    "          \"⊔\":\" OR  \", \"∨\":\" OR  \", \"||\":\" OR  \",\n",
    "          \"¬\":\" NOT \", \"~\":\" NOT \", \"!\":\" NOT \"}\n",
    "BIN_OPS, UNARY_OPS = {\"AND\",\"OR\",\"IMP\",\"IFF\",\"SUB\"}, {\"NOT\"}\n",
    "TOKEN_RE = re.compile(r\"[A-Za-z0-9_]+|[()]\")\n",
    "\n",
    "def norm_text(s):\n",
    "    s = str(s)\n",
    "    for k,v in OP_MAP.items(): s = s.replace(k,v)\n",
    "    return s\n",
    "\n",
    "def lex(s): return TOKEN_RE.findall(norm_text(s))\n",
    "def is_atom(t): return t not in BIN_OPS|UNARY_OPS|{\"(\",\")\"}\n",
    "\n",
    "class Parser:\n",
    "    def __init__(self,toks): self.toks=toks; self.i=0\n",
    "    def peek(self): return self.toks[self.i] if self.i<len(self.toks) else None\n",
    "    def pop(self): t=self.peek(); self.i += (1 if t is not None else 0); return t\n",
    "    def parse(self): return self.expr(0)\n",
    "    PREC = {\"IFF\":1,\"IMP\":2,\"SUB\":2,\"OR\":3,\"AND\":4}\n",
    "    RIGHT = {\"IMP\",\"IFF\",\"SUB\"}\n",
    "    def expr(self,minp):\n",
    "        node=self.unary()\n",
    "        while True:\n",
    "            op=self.peek()\n",
    "            if op in BIN_OPS:\n",
    "                prec=self.PREC.get(op,0)\n",
    "                if prec<minp: break\n",
    "                self.pop()\n",
    "                nextp = prec if op in self.RIGHT else prec+1\n",
    "                rhs=self.expr(nextp)\n",
    "                node=(\"BIN\",op,node,rhs)\n",
    "            else: break\n",
    "        return node\n",
    "    def unary(self):\n",
    "        t=self.peek()\n",
    "        if t in UNARY_OPS:\n",
    "            self.pop(); c=self.unary(); return (\"UN\",t,c)\n",
    "        if t==\"(\":\n",
    "            self.pop(); n=self.expr(0); assert self.pop()==\")\",\"Missing ')'\"\n",
    "            return n\n",
    "        a=self.pop()\n",
    "        return (\"ATOM\", a if a is not None else \"x\")\n",
    "\n",
    "def parse_formula(s):\n",
    "    try: return Parser(lex(s)).parse()\n",
    "    except: return (\"ATOM\",\"x\")\n",
    "\n",
    "def atoms_in(node, acc=None):\n",
    "    if acc is None: acc=set()\n",
    "    k=node[0]\n",
    "    if k==\"ATOM\": acc.add(node[1]); return acc\n",
    "    if k==\"UN\": return atoms_in(node[2], acc)\n",
    "    if k==\"BIN\": atoms_in(node[2], acc); atoms_in(node[3], acc); return acc\n",
    "    return acc\n",
    "\n",
    "def depth(node):\n",
    "    k=node[0]\n",
    "    if k==\"ATOM\": return 1\n",
    "    if k==\"UN\": return 1+depth(node[2])\n",
    "    if k==\"BIN\": return 1+max(depth(node[2]), depth(node[3]))\n",
    "    return 1\n",
    "\n",
    "# --- Deterministic hashing for unseen atoms (probe-consistent) ---\n",
    "def _u64_from_str(s: str) -> int:\n",
    "    h = hashlib.blake2b(s.encode('utf-8'), digest_size=8).digest()\n",
    "    return int.from_bytes(h, 'big')\n",
    "\n",
    "def bernoulli_from_name(atom: str, probe_idx: int, p: float, seed: int) -> bool:\n",
    "    u = (_u64_from_str(f\"{atom}|{probe_idx}|{seed}\") % (1<<53)) / float(1<<53)\n",
    "    return u < p\n",
    "\n",
    "class ProbeEnv:\n",
    "    def __init__(self, base_env: dict, probe_idx: int, bias_p: float, seed: int):\n",
    "        self.base = base_env\n",
    "        self.m    = probe_idx\n",
    "        self.p    = float(bias_p)\n",
    "        self.seed = int(seed)\n",
    "        self.cache = {}\n",
    "    def get(self, atom: str) -> bool:\n",
    "        if atom in self.base: return bool(self.base[atom])\n",
    "        if atom in self.cache: return self.cache[atom]\n",
    "        v = bernoulli_from_name(atom, self.m, self.p, self.seed)\n",
    "        self.cache[atom] = v\n",
    "        return v\n",
    "\n",
    "def eval_ast(node, env_obj):\n",
    "    k=node[0]\n",
    "    if k==\"ATOM\": return bool(env_obj.get(node[1]))\n",
    "    if k==\"UN\":\n",
    "        _,op,c = node\n",
    "        v = eval_ast(c, env_obj)\n",
    "        return (not v)\n",
    "    if k==\"BIN\":\n",
    "        _,op,l,r = node\n",
    "        a = eval_ast(l, env_obj); b = eval_ast(r, env_obj)\n",
    "        if op==\"AND\": return a and b\n",
    "        if op==\"OR\":  return a or b\n",
    "        if op in (\"IMP\",\"SUB\"): return (not a) or b\n",
    "        if op==\"IFF\": return a==b\n",
    "    return False\n",
    "\n",
    "# ---------------- Build semantic fingerprint (FP) ----------------\n",
    "asts = [parse_formula(s) for s in formulas]\n",
    "all_atoms = sorted(set().union(*[atoms_in(t) for t in asts]))\n",
    "A = len(all_atoms)\n",
    "print(f\"#Atoms found: {A}\")\n",
    "\n",
    "# probes (half bias 0.3, half 0.7)\n",
    "rng = np.random.default_rng(SEED)\n",
    "biases = np.concatenate([np.full(M_PROBES//2, 0.3), np.full(M_PROBES - M_PROBES//2, 0.7)])\n",
    "assignments = []\n",
    "for p in biases:\n",
    "    vals = rng.random(A) < p\n",
    "    env = {a: bool(v) for a,v in zip(all_atoms, vals)}  # known atoms only\n",
    "    assignments.append(env)\n",
    "\n",
    "# Truth matrix T: N x M_PROBES with unseen atoms handled via ProbeEnv\n",
    "T_mat = np.zeros((N, M_PROBES), dtype=np.float32)\n",
    "for i,ast in enumerate(asts):\n",
    "    for m_i,base_env in enumerate(assignments):\n",
    "        env_obj = ProbeEnv(base_env, probe_idx=m_i, bias_p=biases[m_i], seed=SEED)\n",
    "        T_mat[i,m_i] = 1.0 if eval_ast(ast, env_obj) else 0.0\n",
    "\n",
    "# Structural features\n",
    "def op_counts(toks):\n",
    "    return toks.count(\"AND\"), toks.count(\"OR\"), toks.count(\"NOT\"), toks.count(\"IMP\")+toks.count(\"SUB\"), toks.count(\"IFF\")\n",
    "struct_rows=[]\n",
    "for s,ast in zip(formulas,asts):\n",
    "    toks = lex(s)\n",
    "    ac = len(atoms_in(ast, set()))\n",
    "    d  = depth(ast)\n",
    "    c_and, c_or, c_not, c_imp, c_iff = op_counts(toks)\n",
    "    struct_rows.append([ac, d, c_and, c_or, c_not, c_imp, c_iff, len(toks)])\n",
    "STRUCT = np.array(struct_rows, dtype=np.float32)\n",
    "\n",
    "# Final FP\n",
    "FP = np.concatenate([T_mat, STRUCT], axis=1).astype(np.float32)\n",
    "print(\"Fingerprint shape:\", FP.shape)\n",
    "\n",
    "# ---------------- FP preprocessing for models ----------------\n",
    "# For GNN encoder features (lowered dimension for stability)\n",
    "sc_fp_gnn = StandardScaler().fit(FP)\n",
    "FP_std_g  = sc_fp_gnn.transform(FP).astype(np.float32)\n",
    "pca_gnn   = PCA(n_components=min(256, FP_std_g.shape[1]), whiten=True, random_state=SEED).fit(FP_std_g)\n",
    "FP_low    = pca_gnn.transform(FP_std_g).astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e5d023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | Loss: 0.013800 | RMSE: 0.116596 | R2: -0.0259 | Pearson: -0.0040 | Spearman: 0.0007\n",
      "Epoch 010 | Loss: 0.013275 | RMSE: 0.115192 | R2: -0.0014 | Pearson: -0.0079 | Spearman: -0.0062\n",
      "Epoch 020 | Loss: 0.013165 | RMSE: 0.115232 | R2: -0.0021 | Pearson: -0.0107 | Spearman: -0.0068\n",
      "Epoch 030 | Loss: 0.012893 | RMSE: 0.115075 | R2: 0.0007 | Pearson: 0.0453 | Spearman: 0.0324\n",
      "Epoch 040 | Loss: 0.012049 | RMSE: 0.112907 | R2: 0.0380 | Pearson: 0.1952 | Spearman: 0.1589\n",
      "Epoch 050 | Loss: 0.009217 | RMSE: 0.099768 | R2: 0.2489 | Pearson: 0.5009 | Spearman: 0.4683\n",
      "Epoch 060 | Loss: 0.005311 | RMSE: 0.074935 | R2: 0.5763 | Pearson: 0.7612 | Spearman: 0.6967\n",
      "Epoch 070 | Loss: 0.004049 | RMSE: 0.065473 | R2: 0.6765 | Pearson: 0.8228 | Spearman: 0.7278\n",
      "Epoch 080 | Loss: 0.003494 | RMSE: 0.061482 | R2: 0.7147 | Pearson: 0.8455 | Spearman: 0.7473\n",
      "Epoch 090 | Loss: 0.003185 | RMSE: 0.059505 | R2: 0.7328 | Pearson: 0.8561 | Spearman: 0.7532\n",
      "Epoch 100 | Loss: 0.002976 | RMSE: 0.058393 | R2: 0.7427 | Pearson: 0.8619 | Spearman: 0.7563\n",
      "Epoch 110 | Loss: 0.002814 | RMSE: 0.057491 | R2: 0.7506 | Pearson: 0.8664 | Spearman: 0.7600\n",
      "Epoch 120 | Loss: 0.002668 | RMSE: 0.056771 | R2: 0.7568 | Pearson: 0.8700 | Spearman: 0.7611\n",
      "Epoch 130 | Loss: 0.002535 | RMSE: 0.056227 | R2: 0.7614 | Pearson: 0.8727 | Spearman: 0.7604\n",
      "Epoch 140 | Loss: 0.002411 | RMSE: 0.055662 | R2: 0.7662 | Pearson: 0.8755 | Spearman: 0.7610\n",
      "Epoch 150 | Loss: 0.002292 | RMSE: 0.055092 | R2: 0.7710 | Pearson: 0.8788 | Spearman: 0.7615\n",
      "Epoch 160 | Loss: 0.002168 | RMSE: 0.054282 | R2: 0.7776 | Pearson: 0.8821 | Spearman: 0.7623\n",
      "Epoch 170 | Loss: 0.002056 | RMSE: 0.053768 | R2: 0.7818 | Pearson: 0.8847 | Spearman: 0.7618\n",
      "Epoch 180 | Loss: 0.001956 | RMSE: 0.053188 | R2: 0.7865 | Pearson: 0.8872 | Spearman: 0.7629\n",
      "Epoch 190 | Loss: 0.001871 | RMSE: 0.052730 | R2: 0.7902 | Pearson: 0.8892 | Spearman: 0.7645\n",
      "Epoch 200 | Loss: 0.001795 | RMSE: 0.052399 | R2: 0.7928 | Pearson: 0.8910 | Spearman: 0.7662\n",
      "Epoch 210 | Loss: 0.001727 | RMSE: 0.052064 | R2: 0.7954 | Pearson: 0.8925 | Spearman: 0.7679\n",
      "Epoch 220 | Loss: 0.001674 | RMSE: 0.051746 | R2: 0.7979 | Pearson: 0.8936 | Spearman: 0.7692\n",
      "Epoch 230 | Loss: 0.001623 | RMSE: 0.051594 | R2: 0.7991 | Pearson: 0.8944 | Spearman: 0.7697\n",
      "Epoch 240 | Loss: 0.001581 | RMSE: 0.051491 | R2: 0.7999 | Pearson: 0.8950 | Spearman: 0.7704\n",
      "Epoch 250 | Loss: 0.001563 | RMSE: 0.051395 | R2: 0.8007 | Pearson: 0.8959 | Spearman: 0.7715\n",
      "Epoch 260 | Loss: 0.001508 | RMSE: 0.051245 | R2: 0.8018 | Pearson: 0.8960 | Spearman: 0.7722\n",
      "Epoch 270 | Loss: 0.001474 | RMSE: 0.051076 | R2: 0.8031 | Pearson: 0.8968 | Spearman: 0.7733\n",
      "Epoch 280 | Loss: 0.001453 | RMSE: 0.051165 | R2: 0.8024 | Pearson: 0.8968 | Spearman: 0.7735\n",
      "Epoch 290 | Loss: 0.001417 | RMSE: 0.051039 | R2: 0.8034 | Pearson: 0.8972 | Spearman: 0.7734\n",
      "Epoch 300 | Loss: 0.001404 | RMSE: 0.051207 | R2: 0.8021 | Pearson: 0.8972 | Spearman: 0.7731\n"
     ]
    }
   ],
   "source": [
    "# ---------------- PyG GNN for Edge Prediction ----------------\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import SAGEConv\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "SEED = 7 \n",
    "\n",
    "# Python random\n",
    "random.seed(SEED)\n",
    "\n",
    "# NumPy\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# PyTorch (CPU)\n",
    "torch.manual_seed(SEED)\n",
    "torch.use_deterministic_algorithms(True)\n",
    "\n",
    "# Node features (N, d)\n",
    "x = torch.tensor(FP_low, dtype=torch.float32)\n",
    "\n",
    "# Training edges (observed)\n",
    "edge_index = torch.tensor(obs_pairs.T, dtype=torch.long)  # (2, E)\n",
    "edge_label = torch.tensor(obs_vals, dtype=torch.float32)\n",
    "\n",
    "# Holdout edges (for validation)\n",
    "val_edge_index = torch.tensor(holdout_pairs.T, dtype=torch.long)\n",
    "val_edge_label = torch.tensor(holdout_true, dtype=torch.float32)\n",
    "\n",
    "data = Data(x=x, edge_index=edge_index)\n",
    "\n",
    "# -------- GNN Encoder  --------\n",
    "class GNNEncoder(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "       \n",
    "        self.conv1 = SAGEConv(-1, hidden_channels)\n",
    "        self.conv2 = SAGEConv( hidden_channels,out_channels)\n",
    "       # self.conv3 = SAGEConv( hidden_channels,out_channels)\n",
    "     \n",
    "    def forward(self, x, edge_index):\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        x1 = self.conv1(x, edge_index).relu()\n",
    "        \n",
    "        x2 = self.conv2(x1, edge_index).relu() + x1\n",
    "        \n",
    "       # x3 = self.conv3(x2, edge_index).relu() +x2\n",
    "    \n",
    "        return x2\n",
    "\n",
    "# -------- Edge Decoder --------\n",
    "class EdgeDecoder(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(2*hidden_channels, hidden_channels),\n",
    "            \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_channels, hidden_channels//2),\n",
    "            \n",
    "\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_channels//2, 1),\n",
    "           \n",
    "            nn.Sigmoid() \n",
    "        )\n",
    "\n",
    "    def forward(self, z, edge_idx):\n",
    "        row, col = edge_idx\n",
    "        zz = torch.cat([ torch.abs(z[row]-z[col]),z[col]*z[row]],dim=-1)\n",
    "        #zz= torch.abs(z[row]-z[col])\n",
    "        return self.mlp(zz).view(-1)\n",
    "\n",
    "# -------- Full Model --------\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        self.encoder = GNNEncoder(hidden_channels, hidden_channels)\n",
    "        self.decoder = EdgeDecoder(hidden_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_label_index):\n",
    "        z = self.encoder(x, edge_index)\n",
    "        return self.decoder(z, edge_label_index)\n",
    "\n",
    "model = Model(hidden_channels=32).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "# -------- Training --------\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    pred = model(data.x, data.edge_index, data.edge_index)\n",
    "    loss = F.mse_loss(pred, edge_label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return float(loss)\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    \n",
    "    # Predict on holdout edges\n",
    "    pred = model(data.x, data.edge_index, val_edge_index)\n",
    "    pred = pred.clamp(0, 1)  # keep in [0,1]\n",
    "\n",
    "    target = val_edge_label\n",
    "\n",
    "    # Convert to numpy\n",
    "    pred_np = pred.numpy()\n",
    "    target_np = target.numpy()\n",
    "\n",
    "    # --- Regression metrics ---\n",
    "    rmse = F.mse_loss(pred, target).sqrt().item()\n",
    "    r2 = r2_score(target_np, pred_np)\n",
    "\n",
    "\n",
    "    # --- Correlation metrics ---\n",
    "    pearson_corr, _ = pearsonr(target_np, pred_np)\n",
    "    spearman_corr, _ = spearmanr(target_np, pred_np)\n",
    "\n",
    "    return rmse, r2,  pearson_corr, spearman_corr\n",
    "\n",
    "# ---------------- TRAINING LOOP ----------------\n",
    "for epoch in range(1, 301):\n",
    "    loss = train()\n",
    "    rmse, r2, pearson_corr, spearman_corr = test()\n",
    "\n",
    "    if epoch % 10 == 0 or epoch == 1:\n",
    "        print(\n",
    "            f\"Epoch {epoch:03d} | \"\n",
    "            f\"Loss: {loss:.6f} | \"\n",
    "            f\"RMSE: {rmse:.6f} | \"\n",
    "            f\"R2: {r2:.4f} | \"\n",
    "            f\"Pearson: {pearson_corr:.4f} | \"\n",
    "            f\"Spearman: {spearman_corr:.4f}\"\n",
    "        )\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scrapy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
