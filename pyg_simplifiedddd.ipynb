{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d9827e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observed (train) pairs=149850 | Holdout (pairs)=349650 | Unobserved=0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "CSV_PATH = r\"C:\\Users\\pc\\Desktop\\30_1000_base.csv\" \n",
    "SEED = 7\n",
    "OBS_FRACTION  = 1\n",
    "HOLDOUT_FRACTION   = 0.7\n",
    "M_PROBES = 256\n",
    "\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "N  = df.shape[0]\n",
    "y = df.iloc[:,0].to_numpy().astype(np.float32)\n",
    "S = df.iloc[:,1:].to_numpy(dtype= float)\n",
    "\n",
    "def upper_pairs(N):\n",
    "    \n",
    "    I, J = np.triu_indices(N, k=1)\n",
    "    return np.stack([I, J], axis=1)\n",
    "\n",
    "all_pairs  = upper_pairs(N)\n",
    "\n",
    "total = len(all_pairs)\n",
    "\n",
    "obs = int(OBS_FRACTION * total)\n",
    "\n",
    "idx = np.random.RandomState(SEED).choice(total, size = obs , replace = False)\n",
    "\n",
    "obs_pairs_all = all_pairs[idx]\n",
    "\n",
    "obs_values_all = S[obs_pairs_all[:,0] ,obs_pairs_all[:,1] ]\n",
    "\n",
    "rng = np.random.RandomState(SEED)\n",
    "\n",
    "perm = rng.permutation(len(obs_pairs_all))\n",
    "\n",
    "hold = int(HOLDOUT_FRACTION * len(obs_pairs_all) )\n",
    "\n",
    "train_obs_idx = perm[hold :]\n",
    "\n",
    "hold_idx = perm[:hold]\n",
    "\n",
    "holdout_pairs = obs_pairs_all[hold_idx]\n",
    "holdout_vals  = obs_values_all [hold_idx]\n",
    "\n",
    "obs_pairs = obs_pairs_all[train_obs_idx]\n",
    "obs_vals = obs_values_all[train_obs_idx]\n",
    "\n",
    "obs_set = set(map(tuple, obs_pairs_all.tolist()))\n",
    "unobs_pairs = np.array([p for p in all_pairs.tolist() if tuple(p) not in obs_set], dtype=np.int64)\n",
    "\n",
    "print(f\"Observed (train) pairs={len(obs_pairs)} | Holdout (pairs)={len(holdout_pairs)} | Unobserved={len(unobs_pairs)}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0225b831",
   "metadata": {},
   "source": [
    "### FP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9a32446f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Atoms found: 30\n",
      "Fingerprint shape: (1000, 264)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import re, hashlib\n",
    "\n",
    "formulas = list(df.columns[1:])\n",
    "# ---------------- Propositional Parser & Robust FP (unseen atoms handled) ----------------\n",
    "OP_MAP = {\"→\":\" IMP \", \"⇒\":\" IMP \", \"=>\":\" IMP \", \"->\":\" IMP \",\n",
    "          \"↔\":\" IFF \", \"<=>\":\" IFF \", \"<->\":\" IFF \",\n",
    "          \"⊑\":\" SUB \",  # treat as IMP\n",
    "          \"⊓\":\" AND \", \"∧\":\" AND \", \"&&\":\" AND \",\n",
    "          \"⊔\":\" OR  \", \"∨\":\" OR  \", \"||\":\" OR  \",\n",
    "          \"¬\":\" NOT \", \"~\":\" NOT \", \"!\":\" NOT \"}\n",
    "BIN_OPS, UNARY_OPS = {\"AND\",\"OR\",\"IMP\",\"IFF\",\"SUB\"}, {\"NOT\"}\n",
    "TOKEN_RE = re.compile(r\"[A-Za-z0-9_]+|[()]\")\n",
    "\n",
    "def norm_text(s):\n",
    "    s = str(s)\n",
    "    for k,v in OP_MAP.items(): s = s.replace(k,v)\n",
    "    return s\n",
    "\n",
    "def lex(s): return TOKEN_RE.findall(norm_text(s))\n",
    "def is_atom(t): return t not in BIN_OPS|UNARY_OPS|{\"(\",\")\"}\n",
    "\n",
    "class Parser:\n",
    "    def __init__(self,toks): self.toks=toks; self.i=0\n",
    "    def peek(self): return self.toks[self.i] if self.i<len(self.toks) else None\n",
    "    def pop(self): t=self.peek(); self.i += (1 if t is not None else 0); return t\n",
    "    def parse(self): return self.expr(0)\n",
    "    PREC = {\"IFF\":1,\"IMP\":2,\"SUB\":2,\"OR\":3,\"AND\":4}\n",
    "    RIGHT = {\"IMP\",\"IFF\",\"SUB\"}\n",
    "    def expr(self,minp):\n",
    "        node=self.unary()\n",
    "        while True:\n",
    "            op=self.peek()\n",
    "            if op in BIN_OPS:\n",
    "                prec=self.PREC.get(op,0)\n",
    "                if prec<minp: break\n",
    "                self.pop()\n",
    "                nextp = prec if op in self.RIGHT else prec+1\n",
    "                rhs=self.expr(nextp)\n",
    "                node=(\"BIN\",op,node,rhs)\n",
    "            else: break\n",
    "        return node\n",
    "    def unary(self):\n",
    "        t=self.peek()\n",
    "        if t in UNARY_OPS:\n",
    "            self.pop(); c=self.unary(); return (\"UN\",t,c)\n",
    "        if t==\"(\":\n",
    "            self.pop(); n=self.expr(0); assert self.pop()==\")\",\"Missing ')'\"\n",
    "            return n\n",
    "        a=self.pop()\n",
    "        return (\"ATOM\", a if a is not None else \"x\")\n",
    "\n",
    "def parse_formula(s):\n",
    "    try: return Parser(lex(s)).parse()\n",
    "    except: return (\"ATOM\",\"x\")\n",
    "\n",
    "def atoms_in(node, acc=None):\n",
    "    if acc is None: acc=set()\n",
    "    k=node[0]\n",
    "    if k==\"ATOM\": acc.add(node[1]); return acc\n",
    "    if k==\"UN\": return atoms_in(node[2], acc)\n",
    "    if k==\"BIN\": atoms_in(node[2], acc); atoms_in(node[3], acc); return acc\n",
    "    return acc\n",
    "\n",
    "def depth(node):\n",
    "    k=node[0]\n",
    "    if k==\"ATOM\": return 1\n",
    "    if k==\"UN\": return 1+depth(node[2])\n",
    "    if k==\"BIN\": return 1+max(depth(node[2]), depth(node[3]))\n",
    "    return 1\n",
    "\n",
    "# --- Deterministic hashing for unseen atoms (probe-consistent) ---\n",
    "def _u64_from_str(s: str) -> int:\n",
    "    h = hashlib.blake2b(s.encode('utf-8'), digest_size=8).digest()\n",
    "    return int.from_bytes(h, 'big')\n",
    "\n",
    "def bernoulli_from_name(atom: str, probe_idx: int, p: float, seed: int) -> bool:\n",
    "    u = (_u64_from_str(f\"{atom}|{probe_idx}|{seed}\") % (1<<53)) / float(1<<53)\n",
    "    return u < p\n",
    "\n",
    "class ProbeEnv:\n",
    "    def __init__(self, base_env: dict, probe_idx: int, bias_p: float, seed: int):\n",
    "        self.base = base_env\n",
    "        self.m    = probe_idx\n",
    "        self.p    = float(bias_p)\n",
    "        self.seed = int(seed)\n",
    "        self.cache = {}\n",
    "    def get(self, atom: str) -> bool:\n",
    "        if atom in self.base: return bool(self.base[atom])\n",
    "        if atom in self.cache: return self.cache[atom]\n",
    "        v = bernoulli_from_name(atom, self.m, self.p, self.seed)\n",
    "        self.cache[atom] = v\n",
    "        return v\n",
    "\n",
    "def eval_ast(node, env_obj):\n",
    "    k=node[0]\n",
    "    if k==\"ATOM\": return bool(env_obj.get(node[1]))\n",
    "    if k==\"UN\":\n",
    "        _,op,c = node\n",
    "        v = eval_ast(c, env_obj)\n",
    "        return (not v)\n",
    "    if k==\"BIN\":\n",
    "        _,op,l,r = node\n",
    "        a = eval_ast(l, env_obj); b = eval_ast(r, env_obj)\n",
    "        if op==\"AND\": return a and b\n",
    "        if op==\"OR\":  return a or b\n",
    "        if op in (\"IMP\",\"SUB\"): return (not a) or b\n",
    "        if op==\"IFF\": return a==b\n",
    "    return False\n",
    "\n",
    "# ---------------- Build semantic fingerprint (FP) ----------------\n",
    "asts = [parse_formula(s) for s in formulas]\n",
    "all_atoms = sorted(set().union(*[atoms_in(t) for t in asts]))\n",
    "A = len(all_atoms)\n",
    "print(f\"#Atoms found: {A}\")\n",
    "\n",
    "# probes (half bias 0.3, half 0.7)\n",
    "rng = np.random.default_rng(SEED)\n",
    "biases = np.concatenate([np.full(M_PROBES//2, 0.3), np.full(M_PROBES - M_PROBES//2, 0.7)])\n",
    "assignments = []\n",
    "for p in biases:\n",
    "    vals = rng.random(A) < p\n",
    "    env = {a: bool(v) for a,v in zip(all_atoms, vals)}  # known atoms only\n",
    "    assignments.append(env)\n",
    "\n",
    "# Truth matrix T: N x M_PROBES with unseen atoms handled via ProbeEnv\n",
    "T_mat = np.zeros((N, M_PROBES), dtype=np.float32)\n",
    "for i,ast in enumerate(asts):\n",
    "    for m_i,base_env in enumerate(assignments):\n",
    "        env_obj = ProbeEnv(base_env, probe_idx=m_i, bias_p=biases[m_i], seed=SEED)\n",
    "        T_mat[i,m_i] = 1.0 if eval_ast(ast, env_obj) else 0.0\n",
    "\n",
    "# Structural features\n",
    "def op_counts(toks):\n",
    "    return toks.count(\"AND\"), toks.count(\"OR\"), toks.count(\"NOT\"), toks.count(\"IMP\")+toks.count(\"SUB\"), toks.count(\"IFF\")\n",
    "struct_rows=[]\n",
    "for s,ast in zip(formulas,asts):\n",
    "    toks = lex(s)\n",
    "    ac = len(atoms_in(ast, set()))\n",
    "    d  = depth(ast)\n",
    "    c_and, c_or, c_not, c_imp, c_iff = op_counts(toks)\n",
    "    struct_rows.append([ac, d, c_and, c_or, c_not, c_imp, c_iff, len(toks)])\n",
    "STRUCT = np.array(struct_rows, dtype=np.float32)\n",
    "\n",
    "# Final FP\n",
    "FP = np.concatenate([T_mat, STRUCT], axis=1).astype(np.float32)\n",
    "print(\"Fingerprint shape:\", FP.shape)\n",
    "\n",
    "# ---------------- FP preprocessing for models ----------------\n",
    "# For GNN encoder features (lowered dimension for stability)\n",
    "sc_fp_gnn = StandardScaler().fit(FP)\n",
    "FP_std_g  = sc_fp_gnn.transform(FP).astype(np.float32)\n",
    "pca_gnn   = PCA(n_components=min(256, FP_std_g.shape[1]), whiten=True, random_state=SEED).fit(FP_std_g)\n",
    "FP_low    = pca_gnn.transform(FP_std_g).astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "999fd664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | Loss: 0.014501 | RMSE: 0.171666 | R2: -1.2215 | Pearson: -0.0040 | Spearman: 0.0041\n",
      "Epoch 010 | Loss: 0.013127 | RMSE: 0.114054 | R2: 0.0194 | Pearson: 0.1712 | Spearman: 0.1534\n",
      "Epoch 020 | Loss: 0.009886 | RMSE: 0.097683 | R2: 0.2807 | Pearson: 0.5955 | Spearman: 0.5358\n",
      "Epoch 030 | Loss: 0.006801 | RMSE: 0.081956 | R2: 0.4937 | Pearson: 0.7031 | Spearman: 0.6706\n",
      "Epoch 040 | Loss: 0.004918 | RMSE: 0.070726 | R2: 0.6229 | Pearson: 0.7915 | Spearman: 0.7185\n",
      "Epoch 050 | Loss: 0.004102 | RMSE: 0.065253 | R2: 0.6790 | Pearson: 0.8247 | Spearman: 0.7328\n",
      "Epoch 060 | Loss: 0.003621 | RMSE: 0.062422 | R2: 0.7063 | Pearson: 0.8419 | Spearman: 0.7443\n",
      "Epoch 070 | Loss: 0.003352 | RMSE: 0.059639 | R2: 0.7319 | Pearson: 0.8562 | Spearman: 0.7527\n",
      "Epoch 080 | Loss: 0.003013 | RMSE: 0.057415 | R2: 0.7515 | Pearson: 0.8672 | Spearman: 0.7594\n",
      "Epoch 090 | Loss: 0.002800 | RMSE: 0.056109 | R2: 0.7627 | Pearson: 0.8739 | Spearman: 0.7620\n",
      "Epoch 100 | Loss: 0.002716 | RMSE: 0.054679 | R2: 0.7746 | Pearson: 0.8814 | Spearman: 0.7661\n",
      "Epoch 110 | Loss: 0.002543 | RMSE: 0.053559 | R2: 0.7838 | Pearson: 0.8869 | Spearman: 0.7697\n",
      "Epoch 120 | Loss: 0.002368 | RMSE: 0.052667 | R2: 0.7909 | Pearson: 0.8914 | Spearman: 0.7729\n",
      "Epoch 130 | Loss: 0.002317 | RMSE: 0.051394 | R2: 0.8009 | Pearson: 0.8952 | Spearman: 0.7746\n",
      "Epoch 140 | Loss: 0.002234 | RMSE: 0.051167 | R2: 0.8026 | Pearson: 0.8972 | Spearman: 0.7749\n",
      "Epoch 150 | Loss: 0.002137 | RMSE: 0.049852 | R2: 0.8127 | Pearson: 0.9018 | Spearman: 0.7799\n",
      "Epoch 160 | Loss: 0.002124 | RMSE: 0.050595 | R2: 0.8070 | Pearson: 0.9030 | Spearman: 0.7802\n",
      "Epoch 170 | Loss: 0.001987 | RMSE: 0.049088 | R2: 0.8184 | Pearson: 0.9063 | Spearman: 0.7835\n",
      "Epoch 180 | Loss: 0.001978 | RMSE: 0.048456 | R2: 0.8230 | Pearson: 0.9078 | Spearman: 0.7836\n",
      "Epoch 190 | Loss: 0.001876 | RMSE: 0.047800 | R2: 0.8278 | Pearson: 0.9098 | Spearman: 0.7853\n",
      "Epoch 200 | Loss: 0.001979 | RMSE: 0.049548 | R2: 0.8149 | Pearson: 0.9094 | Spearman: 0.7842\n",
      "Epoch 210 | Loss: 0.001890 | RMSE: 0.047441 | R2: 0.8303 | Pearson: 0.9130 | Spearman: 0.7884\n",
      "Epoch 220 | Loss: 0.001823 | RMSE: 0.047006 | R2: 0.8334 | Pearson: 0.9145 | Spearman: 0.7893\n",
      "Epoch 230 | Loss: 0.001731 | RMSE: 0.046467 | R2: 0.8372 | Pearson: 0.9159 | Spearman: 0.7910\n",
      "Epoch 240 | Loss: 0.001736 | RMSE: 0.046452 | R2: 0.8373 | Pearson: 0.9162 | Spearman: 0.7897\n",
      "Epoch 250 | Loss: 0.001711 | RMSE: 0.046190 | R2: 0.8392 | Pearson: 0.9181 | Spearman: 0.7931\n",
      "Epoch 260 | Loss: 0.001662 | RMSE: 0.045782 | R2: 0.8420 | Pearson: 0.9185 | Spearman: 0.7916\n",
      "Epoch 270 | Loss: 0.001645 | RMSE: 0.045757 | R2: 0.8422 | Pearson: 0.9201 | Spearman: 0.7942\n",
      "Epoch 280 | Loss: 0.001719 | RMSE: 0.045401 | R2: 0.8446 | Pearson: 0.9206 | Spearman: 0.7938\n",
      "Epoch 290 | Loss: 0.001714 | RMSE: 0.047001 | R2: 0.8335 | Pearson: 0.9174 | Spearman: 0.7869\n",
      "Epoch 300 | Loss: 0.001668 | RMSE: 0.044661 | R2: 0.8496 | Pearson: 0.9225 | Spearman: 0.7963\n",
      "Full similarity matrix shape: torch.Size([1000, 1000])\n",
      "Final node features: torch.Size([1000, 256])\n",
      "=== TRAINING NODE CLASSIFIER ===\n",
      "\n",
      "=== FINAL NODE LABEL METRICS ===\n",
      "Accuracy : 0.8100\n",
      "F1-score : 0.7625\n",
      "Precision: 0.7922\n",
      "Recall   : 0.7349\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ---------------- PyG GNN for Edge Prediction ----------------\n",
    "import torch, torch.nn as nn\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import SAGEConv \n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "import torch.nn.functional as F\n",
    "\n",
    "SEED = 7 \n",
    "\n",
    "# Python random\n",
    "random.seed(SEED)\n",
    "\n",
    "# NumPy\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# PyTorch (CPU)\n",
    "torch.manual_seed(SEED)\n",
    "torch.use_deterministic_algorithms(True)\n",
    "\n",
    "# Node features (N, d)\n",
    "x = torch.tensor(FP_low, dtype=torch.float32)\n",
    "\n",
    "# Training edges (observed)\n",
    "edge_index = torch.tensor(obs_pairs.T, dtype=torch.long) \n",
    "edge_label = torch.tensor(obs_vals, dtype=torch.float32)\n",
    "\n",
    "# Holdout edges (for validation)\n",
    "val_edge_index = torch.tensor(holdout_pairs.T, dtype=torch.long)\n",
    "val_edge_label = torch.tensor(holdout_vals, dtype=torch.float32)\n",
    "\n",
    "data = Data(x=x, edge_index=edge_index)\n",
    "\n",
    "# -------- GNN Encoder  --------\n",
    "class GNNEncoder(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "       \n",
    "        self.conv1 = SAGEConv(-1, hidden_channels)\n",
    "        self.conv2 = SAGEConv(hidden_channels, out_channels)\n",
    "       \n",
    "        \n",
    "    def forward(self, x, edge_index):\n",
    "        x1 = self.conv1(x, edge_index)\n",
    "        x2= self.conv2(x1,edge_index) + x1\n",
    "        return x2\n",
    "\n",
    "# -------- Edge Decoder --------\n",
    "class EdgeDecoder(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            #nn.Linear( 2 * hidden_channels, hidden_channels),\n",
    "            #nn.ReLU(),\n",
    "            nn.Linear(hidden_channels, hidden_channels),\n",
    "            nn.ReLU(), \n",
    "            nn.Linear(hidden_channels, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, z, edge_idx):\n",
    "        row, col = edge_idx\n",
    "        zz = torch.abs(z[row]- z[col])\n",
    "       \n",
    "        ##zz = z[row] *z[col]\n",
    "        \n",
    "        return self.mlp(zz).view(-1)\n",
    "# -------- Full Model --------a\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        self.encoder = GNNEncoder(hidden_channels, hidden_channels)\n",
    "        self.decoder = EdgeDecoder(hidden_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_label_index):\n",
    "        z = self.encoder(x, edge_index)\n",
    "        return self.decoder(z, edge_label_index)\n",
    "\n",
    "model = Model(hidden_channels=32)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "# -------- Training --------\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    pred = model(data.x, data.edge_index, data.edge_index)\n",
    "    loss = F.mse_loss(pred, edge_label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return float(loss)\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    \n",
    "    # Predict on holdout edges\n",
    "    pred = model(data.x, data.edge_index, val_edge_index)\n",
    "    \n",
    "\n",
    "    target = val_edge_label\n",
    "\n",
    "    # Convert to numpy\n",
    "    pred_np = pred.numpy()\n",
    "    target_np = target.numpy()\n",
    "\n",
    "    # --- Regression metrics ---\n",
    "    rmse = F.mse_loss(pred, target).sqrt().item()\n",
    "    r2 = r2_score(target_np, pred_np)\n",
    "\n",
    "\n",
    "    # --- Correlation metrics ---\n",
    "    pearson_corr, _ = pearsonr(target_np, pred_np)\n",
    "    spearman_corr, _ = spearmanr(target_np, pred_np)\n",
    "\n",
    "    return rmse, r2,  pearson_corr, spearman_corr\n",
    "\n",
    "# ---------------- TRAINING LOOP ----------------\n",
    "for epoch in range(1, 301):\n",
    "    loss = train()\n",
    "    rmse, r2, pearson_corr, spearman_corr = test()\n",
    "\n",
    "    if epoch % 10 == 0 or epoch == 1:\n",
    "        print(\n",
    "            f\"Epoch {epoch:03d} | \"\n",
    "            f\"Loss: {loss:.6f} | \"\n",
    "            f\"RMSE: {rmse:.6f} | \"\n",
    "            f\"R2: {r2:.4f} | \"\n",
    "            f\"Pearson: {pearson_corr:.4f} | \"\n",
    "            f\"Spearman: {spearman_corr:.4f}\"\n",
    "        )\n",
    "# ---------------- Compute predicted similarities for all pairs ----------------\n",
    "\n",
    "N = x.size(0)\n",
    "\n",
    "def upper_pairs(N):\n",
    "    \n",
    "    I, J = np.triu_indices(N, k=1)\n",
    "    return np.stack([I, J], axis=1)\n",
    "\n",
    "all_pairs  = upper_pairs(N)\n",
    "\n",
    "obs_set = set((int(i), int(j)) for (i, j) in obs_pairs)\n",
    "\n",
    "missing_pairs_list =[]\n",
    "\n",
    "for i, j in zip(all_pairs[:,0].tolist(), all_pairs[:,1].tolist()):\n",
    "    if (i, j) not in obs_set:\n",
    "        missing_pairs_list.append((i, j))\n",
    "\n",
    "missing_pairs = torch.tensor(missing_pairs_list, dtype=torch.long).T\n",
    "\n",
    "\n",
    "# ------------------ 4. Predict values for MISSING PAIRS ONLY ---------------\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    pred_missing = model(x, edge_index, missing_pairs)\n",
    "   \n",
    "\n",
    "# ------------------ 5. Initialize full N x N similarity matrix -------------\n",
    "full_sim_matrix = torch.zeros((N, N), dtype=torch.float32)\n",
    "\n",
    "# ------------------ 6. Fill observed edges with TRUE values ----------------\n",
    "for (i, j), val in zip(obs_pairs, obs_vals):\n",
    "    full_sim_matrix[i, j] = val\n",
    "    full_sim_matrix[j, i] = val  \n",
    "\n",
    "# ------------------ 7. Fill missing edges with PREDICTED values -----------\n",
    "for (i, j), pred in zip(missing_pairs_list, pred_missing):\n",
    "    full_sim_matrix[i, j] = pred\n",
    "    full_sim_matrix[j, i] = pred  \n",
    "\n",
    "# ------------------ final result: full matrix -------------------------------\n",
    "print(\"Full similarity matrix shape:\", full_sim_matrix.shape)\n",
    "\n",
    "# ---------------- Build final node features ----------------\n",
    "#final_features = torch.cat([x.to(device), full_sim_matrix], dim=1)\n",
    "final_features = x\n",
    "\n",
    "print(\"Final node features:\", final_features.shape)\n",
    "\n",
    "# ---------------- Train/test split ----------------\n",
    "node_labels = torch.tensor(y, dtype=torch.float32)\n",
    "N = node_labels.size(0)\n",
    "perm = torch.randperm(N,)\n",
    "train_size = int(0.8 * N)\n",
    "train_idx = perm[:train_size]\n",
    "test_idx = perm[train_size:]\n",
    "\n",
    "X_train = final_features[train_idx]\n",
    "y_train = node_labels[train_idx]\n",
    "X_test  = final_features[test_idx]\n",
    "y_test  = node_labels[test_idx]\n",
    "\n",
    "# ---------------- Node Classifier ----------------\n",
    "class NodeClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden=256):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, 1),\n",
    "            \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x).view(-1)\n",
    "\n",
    "clf = NodeClassifier(input_dim=final_features.size(1))\n",
    "optimizer2 = torch.optim.Adam(clf.parameters(), lr=1e-3)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# ---------------- Train node classifier ----------------\n",
    "print(\"=== TRAINING NODE CLASSIFIER ===\")\n",
    "for epoch in range(1, 301):\n",
    "    optimizer2.zero_grad()\n",
    "    logits = clf(X_train)\n",
    "    loss = criterion(logits, y_train)\n",
    "    loss.backward()\n",
    "    optimizer2.step()\n",
    "    \n",
    "\n",
    "# ---------------- Evaluate ----------------\n",
    "with torch.no_grad():\n",
    "    logits = clf(X_test)\n",
    "    probs = torch.sigmoid(logits)\n",
    "    y_pred = (probs > 0.5).long().numpy()\n",
    "    y_true = y_test.numpy()\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "\n",
    "print(\"\\n=== FINAL NODE LABEL METRICS ===\")\n",
    "print(f\"Accuracy : {acc:.4f}\")\n",
    "print(f\"F1-score : {f1:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall   : {recall:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scrapy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
